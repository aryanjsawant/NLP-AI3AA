{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d7807b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenized data from Assignment 1...\n",
      "Loaded 500 documents\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from math import log, log10\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load tokenized data from Assignment 1\n",
    "print(\"Loading tokenized data from Assignment 1...\")\n",
    "with open('../lab1/gujarati_corpus_tokenized.pkl', 'rb') as f:\n",
    "    tokenized_data = pickle.load(f)\n",
    "    \n",
    "print(f\"Loaded {len(tokenized_data)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7fb208c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tokenized data with Gujarati tokenizer...\n",
      "Total sentences: 1504\n",
      "Gujarati sentences: 1504\n",
      "Total words: 21965\n",
      "Unique words: 8418\n",
      "\n",
      "Sample sentences:\n",
      "1. આ વીડિયો જુઓ ઊંઝા માર્કેટયાર્ડ આજથી 25 જુલાઈ સુધી બંધ...\n",
      "2. મિથેનોલ આવ્યો ક્યાંથી...\n",
      "3. આખરે ત્રણ રાજ્યોમાં મળેલ હાર પર કોંગ્રેસ અધ્યક્ષ રાહુલ ગાંધી...\n",
      "4. તેમણે કહ્યું કે ત્રિપુરા નાગાલેન્ડ અને મેઘાલયમાં લોકોના જનાદેશનો સ્વાગત...\n",
      "5. આ આંકડો માટે અને વજન ઘટાડવા માટે પ્રકાશનનો દિવસ વિતાવવો...\n",
      "\n",
      "Token type distribution in sample sentences:\n",
      "Token types found:\n",
      "  english_word: 56\n",
      "  gujarati_word: 13939\n",
      "  integer: 258\n",
      "Token types found:\n",
      "  english_word: 56\n",
      "  gujarati_word: 13939\n",
      "  integer: 258\n"
     ]
    }
   ],
   "source": [
    "# Import and use the Gujarati tokenizer\n",
    "import sys\n",
    "sys.path.append('.')  # Add current directory to path\n",
    "from gujarati_tokenizer import GujaratiTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "gujarati_tokenizer = GujaratiTokenizer()\n",
    "\n",
    "# Extract all sentences and words for building language models\n",
    "all_sentences = []\n",
    "all_words = []\n",
    "gujarati_sentences = []  # Only sentences with Gujarati words\n",
    "\n",
    "print(\"Processing tokenized data with Gujarati tokenizer...\")\n",
    "\n",
    "for doc in tokenized_data:\n",
    "    for sentence in doc['sentences']:\n",
    "        words = sentence['words']\n",
    "        classified_words = sentence['classified_words']\n",
    "        \n",
    "        # Re-tokenize and classify using the Gujarati tokenizer for better accuracy\n",
    "        sentence_text = sentence['text']\n",
    "        retokenized_words = gujarati_tokenizer.word_tokenize(sentence_text)\n",
    "        reclassified_words = [(word, gujarati_tokenizer.classify_token(word)) for word in retokenized_words]\n",
    "        \n",
    "        # Filter out punctuation and keep meaningful tokens\n",
    "        meaningful_words = []\n",
    "        has_gujarati = False\n",
    "        \n",
    "        for word, word_type in reclassified_words:\n",
    "            if word_type in ['gujarati_word', 'english_word', 'integer', 'decimal_number']:\n",
    "                meaningful_words.append(word.lower())  # Convert to lowercase for consistency\n",
    "                if word_type == 'gujarati_word':\n",
    "                    has_gujarati = True\n",
    "        \n",
    "        if len(meaningful_words) >= 3:  # Only consider sentences with at least 3 meaningful words\n",
    "            all_sentences.append(meaningful_words)\n",
    "            all_words.extend(meaningful_words)\n",
    "            \n",
    "            if has_gujarati:  # Keep sentences that have at least one Gujarati word\n",
    "                gujarati_sentences.append(meaningful_words)\n",
    "\n",
    "print(f\"Total sentences: {len(all_sentences)}\")\n",
    "print(f\"Gujarati sentences: {len(gujarati_sentences)}\")\n",
    "print(f\"Total words: {len(all_words)}\")\n",
    "print(f\"Unique words: {len(set(all_words))}\")\n",
    "\n",
    "# Display sample sentences\n",
    "print(\"\\nSample sentences:\")\n",
    "for i, sentence in enumerate(gujarati_sentences[:5]):\n",
    "    print(f\"{i+1}. {' '.join(sentence[:10])}...\")\n",
    "\n",
    "# Show token type distribution\n",
    "print(\"\\nToken type distribution in sample sentences:\")\n",
    "token_type_counts = {}\n",
    "sample_size = min(1000, len(gujarati_sentences))\n",
    "\n",
    "for sentence_words in gujarati_sentences[:sample_size]:\n",
    "    # Join words back to text and re-tokenize to get types\n",
    "    sentence_text = ' '.join(sentence_words)\n",
    "    words = gujarati_tokenizer.word_tokenize(sentence_text)\n",
    "    for word in words:\n",
    "        word_type = gujarati_tokenizer.classify_token(word)\n",
    "        if word_type in ['gujarati_word', 'english_word', 'integer', 'decimal_number']:\n",
    "            token_type_counts[word_type] = token_type_counts.get(word_type, 0) + 1\n",
    "\n",
    "print(\"Token types found:\")\n",
    "for token_type, count in sorted(token_type_counts.items()):\n",
    "    print(f\"  {token_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a9b65",
   "metadata": {},
   "source": [
    "## 1. N-gram Language Models\n",
    "\n",
    "We'll implement four different n-gram models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2232e997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:\n",
    "    def __init__(self, n=1):\n",
    "        self.n = n\n",
    "        self.ngram_counts = Counter()\n",
    "        self.context_counts = Counter()  # (n-1)-gram counts for conditional probability\n",
    "        self.vocabulary = set()\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def add_sentence_markers(self, sentence):\n",
    "        \"\"\"Add start and end markers to sentence\"\"\"\n",
    "        return ['<s>'] * (self.n - 1) + sentence + ['</s>']\n",
    "    \n",
    "    def get_ngrams(self, sentence):\n",
    "        \"\"\"Extract n-grams from a sentence\"\"\"\n",
    "        marked_sentence = self.add_sentence_markers(sentence)\n",
    "        ngrams = []\n",
    "        \n",
    "        for i in range(len(marked_sentence) - self.n + 1):\n",
    "            ngram = tuple(marked_sentence[i:i + self.n])\n",
    "            ngrams.append(ngram)\n",
    "            \n",
    "        return ngrams\n",
    "    \n",
    "    def train(self, sentences):\n",
    "        \"\"\"Train the n-gram model on a list of sentences\"\"\"\n",
    "        print(f\"Training {self.n}-gram model on {len(sentences)} sentences...\")\n",
    "        \n",
    "        # Build vocabulary\n",
    "        for sentence in sentences:\n",
    "            self.vocabulary.update(sentence)\n",
    "        \n",
    "        # Add special tokens\n",
    "        self.vocabulary.add('<s>')\n",
    "        self.vocabulary.add('</s>')\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        \n",
    "        # Count n-grams and contexts\n",
    "        for sentence in sentences:\n",
    "            ngrams = self.get_ngrams(sentence)\n",
    "            \n",
    "            for ngram in ngrams:\n",
    "                self.ngram_counts[ngram] += 1\n",
    "                \n",
    "                if self.n > 1:\n",
    "                    context = ngram[:-1]  # (n-1)-gram context\n",
    "                    self.context_counts[context] += 1\n",
    "        \n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"Total {self.n}-grams: {sum(self.ngram_counts.values())}\")\n",
    "        print(f\"Unique {self.n}-grams: {len(self.ngram_counts)}\")\n",
    "        \n",
    "    def probability(self, ngram, smoothing='none', k=1):\n",
    "        \"\"\"Calculate probability of an n-gram with optional smoothing\"\"\"\n",
    "        if isinstance(ngram, list):\n",
    "            ngram = tuple(ngram)\n",
    "            \n",
    "        if self.n == 1:\n",
    "            # Unigram probability\n",
    "            count = self.ngram_counts[ngram]\n",
    "            total = sum(self.ngram_counts.values())\n",
    "            \n",
    "            if smoothing == 'add_one':\n",
    "                return (count + 1) / (total + self.vocab_size)\n",
    "            elif smoothing == 'add_k':\n",
    "                return (count + k) / (total + k * self.vocab_size)\n",
    "            elif smoothing == 'add_token_type':\n",
    "                unique_types = len(self.ngram_counts)\n",
    "                return (count + 1) / (total + unique_types)\n",
    "            else:\n",
    "                return count / total if total > 0 else 0\n",
    "        else:\n",
    "            # Higher-order n-gram probability P(w_n | w_1...w_{n-1})\n",
    "            context = ngram[:-1]\n",
    "            count = self.ngram_counts[ngram]\n",
    "            context_count = self.context_counts[context]\n",
    "            \n",
    "            if smoothing == 'add_one':\n",
    "                return (count + 1) / (context_count + self.vocab_size)\n",
    "            elif smoothing == 'add_k':\n",
    "                return (count + k) / (context_count + k * self.vocab_size)\n",
    "            elif smoothing == 'add_token_type':\n",
    "                unique_types = len([ng for ng in self.ngram_counts.keys() if ng[:-1] == context])\n",
    "                if unique_types == 0:\n",
    "                    unique_types = 1\n",
    "                return (count + 1) / (context_count + unique_types)\n",
    "            else:\n",
    "                return count / context_count if context_count > 0 else 0\n",
    "    \n",
    "    def sentence_probability(self, sentence, smoothing='none', k=1):\n",
    "        \"\"\"Calculate probability of a sentence\"\"\"\n",
    "        ngrams = self.get_ngrams(sentence)\n",
    "        log_prob = 0\n",
    "        \n",
    "        for ngram in ngrams:\n",
    "            prob = self.probability(ngram, smoothing, k)\n",
    "            if prob > 0:\n",
    "                log_prob += log10(prob)\n",
    "            else:\n",
    "                return float('-inf')  # Zero probability\n",
    "                \n",
    "        return log_prob\n",
    "    \n",
    "    def perplexity(self, test_sentences, smoothing='none', k=1):\n",
    "        \"\"\"Calculate perplexity on test sentences\"\"\"\n",
    "        total_log_prob = 0\n",
    "        total_words = 0\n",
    "        \n",
    "        for sentence in test_sentences:\n",
    "            log_prob = self.sentence_probability(sentence, smoothing, k)\n",
    "            if log_prob != float('-inf'):\n",
    "                total_log_prob += log_prob\n",
    "                total_words += len(sentence) + 1  # +1 for </s> token\n",
    "        \n",
    "        if total_words == 0:\n",
    "            return float('inf')\n",
    "            \n",
    "        avg_log_prob = total_log_prob / total_words\n",
    "        return 10 ** (-avg_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9203fa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training n-gram models...\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Unigram Model (n=1)\n",
      "--------------------------------------------------\n",
      "Training 1-gram model on 1504 sentences...\n",
      "Vocabulary size: 8420\n",
      "Total 1-grams: 23469\n",
      "Unique 1-grams: 8419\n",
      "\n",
      "Most common unigrams:\n",
      "  </s>: 1504\n",
      "  છે: 937\n",
      "  અને: 445\n",
      "  આ: 276\n",
      "  પણ: 220\n",
      "  કે: 213\n",
      "  માટે: 206\n",
      "  કરી: 142\n",
      "  પર: 131\n",
      "  જ: 127\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Bigram Model (n=2)\n",
      "--------------------------------------------------\n",
      "Training 2-gram model on 1504 sentences...\n",
      "Vocabulary size: 8420\n",
      "Total 2-grams: 23469\n",
      "Unique 2-grams: 19100\n",
      "\n",
      "Most common bigrams:\n",
      "  છે </s>: 607\n",
      "  <s> આ: 125\n",
      "  હતી </s>: 85\n",
      "  હતો </s>: 62\n",
      "  છે અને: 59\n",
      "  છે કે: 59\n",
      "  હતા </s>: 51\n",
      "  શકે છે: 48\n",
      "  કરે છે: 45\n",
      "  હતું </s>: 38\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Trigram Model (n=3)\n",
      "--------------------------------------------------\n",
      "Training 3-gram model on 1504 sentences...\n",
      "Vocabulary size: 8420\n",
      "Total 3-grams: 23469\n",
      "Unique 3-grams: 21774\n",
      "\n",
      "Most common trigrams:\n",
      "  <s> <s> આ: 125\n",
      "  શકે છે </s>: 37\n",
      "  કરે છે </s>: 29\n",
      "  આવી છે </s>: 24\n",
      "  <s> <s> જો: 22\n",
      "  આવે છે </s>: 20\n",
      "  <s> <s> એક: 19\n",
      "  થાય છે </s>: 18\n",
      "  રહી છે </s>: 18\n",
      "  રહ્યા છે </s>: 17\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Quadrigram Model (n=4)\n",
      "--------------------------------------------------\n",
      "Training 4-gram model on 1504 sentences...\n",
      "Vocabulary size: 8420\n",
      "Total 4-grams: 23469\n",
      "Unique 4-grams: 22529\n",
      "\n",
      "Most common quadrigrams:\n",
      "  <s> <s> <s> આ: 125\n",
      "  <s> <s> <s> જો: 22\n",
      "  <s> <s> <s> એક: 19\n",
      "  <s> <s> <s> તે: 15\n",
      "  <s> <s> <s> જોકે: 15\n",
      "  <s> <s> <s> પરંતુ: 15\n",
      "  કરવામાં આવી છે </s>: 13\n",
      "  કરી શકે છે </s>: 13\n",
      "  <s> <s> <s> તેમણે: 11\n",
      "  <s> <s> <s> જ્યારે: 11\n",
      "\n",
      "============================================================\n",
      "All models trained successfully!\n",
      "============================================================\n",
      "Vocabulary size: 8420\n",
      "Total 2-grams: 23469\n",
      "Unique 2-grams: 19100\n",
      "\n",
      "Most common bigrams:\n",
      "  છે </s>: 607\n",
      "  <s> આ: 125\n",
      "  હતી </s>: 85\n",
      "  હતો </s>: 62\n",
      "  છે અને: 59\n",
      "  છે કે: 59\n",
      "  હતા </s>: 51\n",
      "  શકે છે: 48\n",
      "  કરે છે: 45\n",
      "  હતું </s>: 38\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Trigram Model (n=3)\n",
      "--------------------------------------------------\n",
      "Training 3-gram model on 1504 sentences...\n",
      "Vocabulary size: 8420\n",
      "Total 3-grams: 23469\n",
      "Unique 3-grams: 21774\n",
      "\n",
      "Most common trigrams:\n",
      "  <s> <s> આ: 125\n",
      "  શકે છે </s>: 37\n",
      "  કરે છે </s>: 29\n",
      "  આવી છે </s>: 24\n",
      "  <s> <s> જો: 22\n",
      "  આવે છે </s>: 20\n",
      "  <s> <s> એક: 19\n",
      "  થાય છે </s>: 18\n",
      "  રહી છે </s>: 18\n",
      "  રહ્યા છે </s>: 17\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Quadrigram Model (n=4)\n",
      "--------------------------------------------------\n",
      "Training 4-gram model on 1504 sentences...\n",
      "Vocabulary size: 8420\n",
      "Total 4-grams: 23469\n",
      "Unique 4-grams: 22529\n",
      "\n",
      "Most common quadrigrams:\n",
      "  <s> <s> <s> આ: 125\n",
      "  <s> <s> <s> જો: 22\n",
      "  <s> <s> <s> એક: 19\n",
      "  <s> <s> <s> તે: 15\n",
      "  <s> <s> <s> જોકે: 15\n",
      "  <s> <s> <s> પરંતુ: 15\n",
      "  કરવામાં આવી છે </s>: 13\n",
      "  કરી શકે છે </s>: 13\n",
      "  <s> <s> <s> તેમણે: 11\n",
      "  <s> <s> <s> જ્યારે: 11\n",
      "\n",
      "============================================================\n",
      "All models trained successfully!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Train all four n-gram models\n",
    "models = {}\n",
    "n_values = [1, 2, 3, 4]\n",
    "model_names = ['Unigram', 'Bigram', 'Trigram', 'Quadrigram']\n",
    "\n",
    "print(\"Training n-gram models...\\n\")\n",
    "\n",
    "for n, name in zip(n_values, model_names):\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"Training {name} Model (n={n})\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    \n",
    "    model = NGramLanguageModel(n=n)\n",
    "    model.train(gujarati_sentences)\n",
    "    models[name] = model\n",
    "    \n",
    "    # Show some example n-grams\n",
    "    print(f\"\\nMost common {name.lower()}s:\")\n",
    "    for ngram, count in model.ngram_counts.most_common(10):\n",
    "        print(f\"  {' '.join(ngram)}: {count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All models trained successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a2564a",
   "metadata": {},
   "source": [
    "## 2. Smoothing Techniques\n",
    "\n",
    "Now let's test different smoothing techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bf941d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Smoothing Techniques\n",
      "================================================================================\n",
      "\n",
      "Sentence: આજે હું શાળાએ ગયો\n",
      "------------------------------------------------------------\n",
      "\n",
      "Unigram Model:\n",
      "  No Smoothing             : -∞ (zero probability)\n",
      "  Add-One Smoothing        : -15.678855\n",
      "  Add-K Smoothing (k=0.5)  : -15.717665\n",
      "  Add Token Type Smoothing : -15.678787\n",
      "\n",
      "Bigram Model:\n",
      "  No Smoothing             : -∞ (zero probability)\n",
      "  Add-One Smoothing        : -18.320454\n",
      "  Add-K Smoothing (k=0.5)  : -17.925823\n",
      "  Add Token Type Smoothing : -6.278849\n",
      "\n",
      "Trigram Model:\n",
      "  No Smoothing             : -∞ (zero probability)\n",
      "  Add-One Smoothing        : -18.619321\n",
      "  Add-K Smoothing (k=0.5)  : -18.398624\n",
      "  Add Token Type Smoothing : -3.647774\n",
      "\n",
      "Quadrigram Model:\n",
      "  No Smoothing             : -∞ (zero probability)\n",
      "  Add-One Smoothing        : -18.619321\n",
      "  Add-K Smoothing (k=0.5)  : -18.398624\n",
      "  Add Token Type Smoothing : -3.647774\n",
      "\n",
      "Sentence: મારું નામ અર્જુન છે\n",
      "------------------------------------------------------------\n",
      "\n",
      "Unigram Model:\n",
      "  No Smoothing             : -14.624304\n",
      "  Add-One Smoothing        : -14.652462\n",
      "  Add-K Smoothing (k=0.5)  : -14.612296\n",
      "  Add Token Type Smoothing : -14.652394\n",
      "\n",
      "Bigram Model:\n",
      "  No Smoothing             : -∞ (zero probability)\n",
      "  Add-One Smoothing        : -16.960578\n",
      "  Add-K Smoothing (k=0.5)  : -16.763357\n",
      "  Add Token Type Smoothing : -5.602432\n",
      "\n",
      "Trigram Model:\n",
      "  No Smoothing             : -∞ (zero probability)\n",
      "  Add-One Smoothing        : -19.697935\n",
      "  Add-K Smoothing (k=0.5)  : -19.759219\n",
      "  Add Token Type Smoothing : -3.384533\n",
      "\n",
      "Quadrigram Model:\n",
      "  No Smoothing             : -∞ (zero probability)\n",
      "  Add-One Smoothing        : -19.697935\n",
      "  Add-K Smoothing (k=0.5)  : -19.759219\n",
      "  Add Token Type Smoothing : -3.384533\n",
      "\n",
      "Sentence: ગુજરાત એક સુંદર રાજ્ય છે\n",
      "------------------------------------------------------------\n",
      "\n",
      "Unigram Model:\n",
      "  No Smoothing             : -15.323858\n",
      "  Add-One Smoothing        : -15.879641\n",
      "  Add-K Smoothing (k=0.5)  : -15.622691\n",
      "  Add Token Type Smoothing : -15.879559\n",
      "\n",
      "Bigram Model:\n",
      "  No Smoothing             : -∞ (zero probability)\n",
      "  Add-One Smoothing        : -20.115272\n",
      "  Add-K Smoothing (k=0.5)  : -19.662246\n",
      "  Add Token Type Smoothing : -8.627465\n",
      "\n",
      "Trigram Model:\n",
      "  No Smoothing             : -∞ (zero probability)\n",
      "  Add-One Smoothing        : -22.845354\n",
      "  Add-K Smoothing (k=0.5)  : -22.643653\n",
      "  Add Token Type Smoothing : -3.606381\n",
      "\n",
      "Quadrigram Model:\n",
      "  No Smoothing             : -∞ (zero probability)\n",
      "  Add-One Smoothing        : -22.845354\n",
      "  Add-K Smoothing (k=0.5)  : -22.643653\n",
      "  Add Token Type Smoothing : -3.606381\n",
      "  Add Token Type Smoothing : -3.384533\n",
      "\n",
      "Sentence: ગુજરાત એક સુંદર રાજ્ય છે\n",
      "------------------------------------------------------------\n",
      "\n",
      "Unigram Model:\n",
      "  No Smoothing             : -15.323858\n",
      "  Add-One Smoothing        : -15.879641\n",
      "  Add-K Smoothing (k=0.5)  : -15.622691\n",
      "  Add Token Type Smoothing : -15.879559\n",
      "\n",
      "Bigram Model:\n",
      "  No Smoothing             : -∞ (zero probability)\n",
      "  Add-One Smoothing        : -20.115272\n",
      "  Add-K Smoothing (k=0.5)  : -19.662246\n",
      "  Add Token Type Smoothing : -8.627465\n",
      "\n",
      "Trigram Model:\n",
      "  No Smoothing             : -∞ (zero probability)\n",
      "  Add-One Smoothing        : -22.845354\n",
      "  Add-K Smoothing (k=0.5)  : -22.643653\n",
      "  Add Token Type Smoothing : -3.606381\n",
      "\n",
      "Quadrigram Model:\n",
      "  No Smoothing             : -∞ (zero probability)\n",
      "  Add-One Smoothing        : -22.845354\n",
      "  Add-K Smoothing (k=0.5)  : -22.643653\n",
      "  Add Token Type Smoothing : -3.606381\n"
     ]
    }
   ],
   "source": [
    "# Test smoothing techniques with sample sentences\n",
    "test_sentences = [\n",
    "    ['આજે', 'હું', 'શાળાએ', 'ગયો'],\n",
    "    ['મારું', 'નામ', 'અર્જુન', 'છે'],\n",
    "    ['ગુજરાત', 'એક', 'સુંદર', 'રાજ્ય', 'છે']\n",
    "]\n",
    "\n",
    "smoothing_techniques = {\n",
    "    'No Smoothing': ('none', 1),\n",
    "    'Add-One Smoothing': ('add_one', 1),\n",
    "    'Add-K Smoothing (k=0.5)': ('add_k', 0.5),\n",
    "    'Add Token Type Smoothing': ('add_token_type', 1)\n",
    "}\n",
    "\n",
    "print(\"Testing Smoothing Techniques\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    print(f\"\\nSentence: {' '.join(sentence)}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n{model_name} Model:\")\n",
    "        \n",
    "        for smooth_name, (smooth_type, k) in smoothing_techniques.items():\n",
    "            prob = model.sentence_probability(sentence, smoothing=smooth_type, k=k)\n",
    "            if prob == float('-inf'):\n",
    "                print(f\"  {smooth_name:25}: -∞ (zero probability)\")\n",
    "            else:\n",
    "                print(f\"  {smooth_name:25}: {prob:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6142f7a",
   "metadata": {},
   "source": [
    "## 3. Generate Test Sentences and Compute Probabilities\n",
    "\n",
    "Since we don't have access to external news articles, we'll create test sentences by:\n",
    "1. Sampling random sentences from our corpus\n",
    "2. Creating variations of existing sentences\n",
    "3. Generating some out-of-vocabulary test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d9d9a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test sentences...\n",
      "Generated 998 test sentences\n",
      "\n",
      "Sample test sentences:\n",
      " 1. ઉલ્લેખનીય છે કે હાલમાં આ વિસ્તારના કાયદા અને...\n",
      " 2. બાકી તો પાડા પર પાણી રેડવા સમાન ચામડીના...\n",
      " 3. આમ સરકારી હોસ્પિટલમાં પણ જો યોગ્ય સારવાર ન...\n",
      " 4. જોર્જ ફ્લોયડની ઘટનાઓ\n",
      " 5. આપણાં શાસ્ત્રોમાં ધનતેરસ વિશે ઘણીબધી ચર્ચા કરવામાં આવેલી...\n",
      " 6. જો તમને ખાતરી નથી કે તમારી લોન એસબીએ...\n",
      " 7. યુનાઇટેડ સ્ટેટ્સ બ્યુરો ઓફ લેબર સ્ટેટિસ્ટિક્સ અનુસાર 2008...\n",
      " 8. આવી રીતે માછીમારો પોતાના પગ ઉપર જ કુહાડો...\n",
      " 9. સબસ્ટ્રેટને મદદથી માળ સપાટી અસરકારક કામગીરી દરમિયાન અસર...\n",
      "10. નવી સેન્ટ્રોમા રિયલ એસી પણ આપવામાં આવ્યુ છે\n"
     ]
    }
   ],
   "source": [
    "# Generate test sentences\n",
    "def generate_test_sentences(sentences, num_sentences=1000):\n",
    "    \"\"\"Generate test sentences for evaluation\"\"\"\n",
    "    test_sentences = []\n",
    "    \n",
    "    # 1. Random sampling from corpus (70%)\n",
    "    random_sample = random.sample(sentences, min(700, len(sentences)))\n",
    "    test_sentences.extend(random_sample)\n",
    "    \n",
    "    # 2. Sentence variations - shuffle words (20%)\n",
    "    variations = []\n",
    "    for _ in range(200):\n",
    "        original = random.choice(sentences)\n",
    "        if len(original) > 2:\n",
    "            shuffled = original.copy()\n",
    "            random.shuffle(shuffled)\n",
    "            variations.append(shuffled)\n",
    "    test_sentences.extend(variations)\n",
    "    \n",
    "    # 3. Partial sentences (10%)\n",
    "    partial_sentences = []\n",
    "    for _ in range(100):\n",
    "        original = random.choice(sentences)\n",
    "        if len(original) > 3:\n",
    "            # Take random substring\n",
    "            start = random.randint(0, len(original) - 3)\n",
    "            end = random.randint(start + 2, len(original))\n",
    "            partial_sentences.append(original[start:end])\n",
    "    test_sentences.extend(partial_sentences)\n",
    "    \n",
    "    return test_sentences[:num_sentences]\n",
    "\n",
    "# Generate test sentences\n",
    "print(\"Generating test sentences...\")\n",
    "test_sentences = generate_test_sentences(gujarati_sentences, 1000)\n",
    "print(f\"Generated {len(test_sentences)} test sentences\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nSample test sentences:\")\n",
    "for i, sentence in enumerate(test_sentences[:10]):\n",
    "    print(f\"{i+1:2d}. {' '.join(sentence[:8])}{'...' if len(sentence) > 8 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7af5048d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive evaluation...\n",
      "Evaluating models on 998 test sentences...\n",
      "\n",
      "Evaluating Unigram model...\n",
      "    No Smoothing: Avg log prob = -48.5552, Std = 32.7218, Zero probs = 0\n",
      "  Progress: 1000/15968 (6.3%)\n",
      "    No Smoothing: Avg log prob = -48.5552, Std = 32.7218, Zero probs = 0\n",
      "  Progress: 1000/15968 (6.3%)\n",
      "    Add-One Smoothing: Avg log prob = -48.7849, Std = 32.7225, Zero probs = 0\n",
      "  Progress: 2000/15968 (12.5%)\n",
      "    Add-One Smoothing: Avg log prob = -48.7849, Std = 32.7225, Zero probs = 0\n",
      "  Progress: 2000/15968 (12.5%)\n",
      "    Add-K Smoothing (k=0.5): Avg log prob = -48.6293, Std = 32.6851, Zero probs = 0\n",
      "  Progress: 3000/15968 (18.8%)\n",
      "    Add-K Smoothing (k=0.5): Avg log prob = -48.6293, Std = 32.6851, Zero probs = 0\n",
      "  Progress: 3000/15968 (18.8%)\n",
      "    Add Token Type Smoothing: Avg log prob = -48.7847, Std = 32.7224, Zero probs = 0\n",
      "\n",
      "Evaluating Bigram model...\n",
      "  Progress: 4000/15968 (25.1%)\n",
      "    No Smoothing: Avg log prob = -12.0746, Std = 7.1155, Zero probs = 288\n",
      "  Progress: 5000/15968 (31.3%)\n",
      "    Add-One Smoothing: Avg log prob = -51.7756, Std = 33.7942, Zero probs = 0\n",
      "  Progress: 6000/15968 (37.6%)\n",
      "    Add-K Smoothing (k=0.5): Avg log prob = -49.6056, Std = 32.7170, Zero probs = 0\n",
      "    Add Token Type Smoothing: Avg log prob = -48.7847, Std = 32.7224, Zero probs = 0\n",
      "\n",
      "Evaluating Bigram model...\n",
      "  Progress: 4000/15968 (25.1%)\n",
      "    No Smoothing: Avg log prob = -12.0746, Std = 7.1155, Zero probs = 288\n",
      "  Progress: 5000/15968 (31.3%)\n",
      "    Add-One Smoothing: Avg log prob = -51.7756, Std = 33.7942, Zero probs = 0\n",
      "  Progress: 6000/15968 (37.6%)\n",
      "    Add-K Smoothing (k=0.5): Avg log prob = -49.6056, Std = 32.7170, Zero probs = 0\n",
      "  Progress: 7000/15968 (43.8%)\n",
      "  Progress: 7000/15968 (43.8%)\n",
      "    Add Token Type Smoothing: Avg log prob = -13.2929, Std = 8.7573, Zero probs = 0\n",
      "\n",
      "Evaluating Trigram model...\n",
      "  Progress: 8000/15968 (50.1%)\n",
      "    No Smoothing: Avg log prob = -4.1385, Std = 1.3214, Zero probs = 294\n",
      "  Progress: 9000/15968 (56.4%)\n",
      "    Add-One Smoothing: Avg log prob = -53.4394, Std = 34.3707, Zero probs = 0\n",
      "  Progress: 10000/15968 (62.6%)\n",
      "    Add-K Smoothing (k=0.5): Avg log prob = -51.4435, Std = 33.3296, Zero probs = 0\n",
      "    Add Token Type Smoothing: Avg log prob = -13.2929, Std = 8.7573, Zero probs = 0\n",
      "\n",
      "Evaluating Trigram model...\n",
      "  Progress: 8000/15968 (50.1%)\n",
      "    No Smoothing: Avg log prob = -4.1385, Std = 1.3214, Zero probs = 294\n",
      "  Progress: 9000/15968 (56.4%)\n",
      "    Add-One Smoothing: Avg log prob = -53.4394, Std = 34.3707, Zero probs = 0\n",
      "  Progress: 10000/15968 (62.6%)\n",
      "    Add-K Smoothing (k=0.5): Avg log prob = -51.4435, Std = 33.3296, Zero probs = 0\n",
      "  Progress: 11000/15968 (68.9%)\n",
      "  Progress: 11000/15968 (68.9%)\n",
      "    Add Token Type Smoothing: Avg log prob = -4.1020, Std = 1.1562, Zero probs = 0\n",
      "\n",
      "Evaluating Quadrigram model...\n",
      "  Progress: 12000/15968 (75.2%)\n",
      "    No Smoothing: Avg log prob = -3.2945, Std = 0.2712, Zero probs = 294\n",
      "  Progress: 13000/15968 (81.4%)\n",
      "    Add-One Smoothing: Avg log prob = -53.7030, Std = 34.4354, Zero probs = 0\n",
      "  Progress: 14000/15968 (87.7%)\n",
      "    Add-K Smoothing (k=0.5): Avg log prob = -51.7607, Std = 33.3939, Zero probs = 0\n",
      "    Add Token Type Smoothing: Avg log prob = -4.1020, Std = 1.1562, Zero probs = 0\n",
      "\n",
      "Evaluating Quadrigram model...\n",
      "  Progress: 12000/15968 (75.2%)\n",
      "    No Smoothing: Avg log prob = -3.2945, Std = 0.2712, Zero probs = 294\n",
      "  Progress: 13000/15968 (81.4%)\n",
      "    Add-One Smoothing: Avg log prob = -53.7030, Std = 34.4354, Zero probs = 0\n",
      "  Progress: 14000/15968 (87.7%)\n",
      "    Add-K Smoothing (k=0.5): Avg log prob = -51.7607, Std = 33.3939, Zero probs = 0\n",
      "  Progress: 15000/15968 (93.9%)\n",
      "  Progress: 15000/15968 (93.9%)\n",
      "    Add Token Type Smoothing: Avg log prob = -3.3622, Std = 0.2816, Zero probs = 0\n",
      "\n",
      "Evaluation completed!\n",
      "    Add Token Type Smoothing: Avg log prob = -3.3622, Std = 0.2816, Zero probs = 0\n",
      "\n",
      "Evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "# Compute probabilities for all test sentences with different models and smoothing\n",
    "def evaluate_models_on_test_set(models, test_sentences, smoothing_techniques):\n",
    "    \"\"\"Evaluate all models on test sentences with different smoothing techniques\"\"\"\n",
    "    \n",
    "    results = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    print(f\"Evaluating models on {len(test_sentences)} test sentences...\")\n",
    "    \n",
    "    total_evaluations = len(models) * len(smoothing_techniques) * len(test_sentences)\n",
    "    current_eval = 0\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nEvaluating {model_name} model...\")\n",
    "        \n",
    "        for smooth_name, (smooth_type, k) in smoothing_techniques.items():\n",
    "            sentence_probs = []\n",
    "            \n",
    "            for i, sentence in enumerate(test_sentences):\n",
    "                prob = model.sentence_probability(sentence, smoothing=smooth_type, k=k)\n",
    "                sentence_probs.append(prob)\n",
    "                \n",
    "                current_eval += 1\n",
    "                if current_eval % 1000 == 0:\n",
    "                    print(f\"  Progress: {current_eval}/{total_evaluations} ({current_eval/total_evaluations*100:.1f}%)\")\n",
    "            \n",
    "            results[model_name][smooth_name] = sentence_probs\n",
    "            \n",
    "            # Calculate statistics\n",
    "            valid_probs = [p for p in sentence_probs if p != float('-inf')]\n",
    "            zero_prob_count = len(sentence_probs) - len(valid_probs)\n",
    "            \n",
    "            if valid_probs:\n",
    "                avg_log_prob = np.mean(valid_probs)\n",
    "                std_log_prob = np.std(valid_probs)\n",
    "                print(f\"    {smooth_name}: Avg log prob = {avg_log_prob:.4f}, Std = {std_log_prob:.4f}, Zero probs = {zero_prob_count}\")\n",
    "            else:\n",
    "                print(f\"    {smooth_name}: All probabilities are zero!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Starting comprehensive evaluation...\")\n",
    "results = evaluate_models_on_test_set(models, test_sentences, smoothing_techniques)\n",
    "print(\"\\nEvaluation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
