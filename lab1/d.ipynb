{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e808cd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aryan\\Documents\\GitHub\\nlp-ai3aa\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries and tokenizer imported successfully!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from gujarati_tokenizer import GujaratiTokenizer, process_dataset, save_tokenized_data, compute_corpus_statistics\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Libraries and tokenizer imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e2596ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gujarati dataset from IndicCorpV2...\n",
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Gujarati dataset from IndicCorpV2...\")\n",
    "dataset = load_dataset(\"ai4bharat/IndicCorpV2\", \"indiccorp_v2\", split=\"guj_Gujr\", streaming=True)\n",
    "print(\"Dataset loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c3acb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing tokenizer with sample Gujarati text...\n",
      "Original text:\n",
      "આજે હું શાળાએ ગયો! મારું ઇમેઇલ student@gmail.com છે। \n",
      "આજની તારીખ 30/07/2025 છે અને સમય 10.30 વાગ્યે છે। \n",
      "વેબસાઇટ www.gujarati.com પર જાઓ। આ કિંમત ₹123.45 છે।\n",
      "\n",
      "==================================================\n",
      "Number of sentences: 10\n",
      "\n",
      "Sentences:\n",
      "1. આજે હું શાળાએ ગયો\n",
      "2. મારું ઇમેઇલ student@gmail\n",
      "3. com છે\n",
      "4. આજની તારીખ 30/07/2025 છે અને સમય 10\n",
      "5. 30 વાગ્યે છે\n",
      "6. વેબસાઇટ www\n",
      "7. gujarati\n",
      "8. com પર જાઓ\n",
      "9. આ કિંમત ₹123\n",
      "10. 45 છે\n",
      "\n",
      "==================================================\n",
      "Number of tokens: 34\n",
      "\n",
      "Tokens with classification:\n",
      "'આજે' -> gujarati_word\n",
      "'હું' -> gujarati_word\n",
      "'શાળાએ' -> gujarati_word\n",
      "'ગયો' -> gujarati_word\n",
      "'!' -> punctuation\n",
      "'મારું' -> gujarati_word\n",
      "'ઇમેઇલ' -> gujarati_word\n",
      "'student@gmail.com' -> email\n",
      "'છે' -> gujarati_word\n",
      "'।' -> punctuation\n",
      "'આજની' -> gujarati_word\n",
      "'તારીખ' -> gujarati_word\n",
      "'30' -> integer\n",
      "'/' -> punctuation\n",
      "'07' -> integer\n",
      "'/' -> punctuation\n",
      "'2025' -> integer\n",
      "'છે' -> gujarati_word\n",
      "'અને' -> gujarati_word\n",
      "'સમય' -> gujarati_word\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Test the tokenizer with sample data\n",
    "print(\"Testing tokenizer with sample Gujarati text...\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = GujaratiTokenizer()\n",
    "\n",
    "# Test with sample Gujarati text containing various elements\n",
    "test_text = \"\"\"આજે હું શાળાએ ગયો! મારું ઇમેઇલ student@gmail.com છે। \n",
    "આજની તારીખ 30/07/2025 છે અને સમય 10.30 વાગ્યે છે। \n",
    "વેબસાઇટ www.gujarati.com પર જાઓ। આ કિંમત ₹123.45 છે।\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(test_text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Test sentence tokenization\n",
    "sentences = tokenizer.sentence_tokenize(test_text)\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "print(\"\\nSentences:\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sent.strip()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Test word tokenization\n",
    "words = tokenizer.word_tokenize(test_text)\n",
    "print(f\"Number of tokens: {len(words)}\")\n",
    "print(\"\\nTokens with classification:\")\n",
    "for word in words[:20]:  # Show first 20 tokens\n",
    "    word_type = tokenizer.classify_token(word)\n",
    "    print(f\"'{word}' -> {word_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eb43ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the dataset...\n",
      "Note: Processing 1000 examples. Adjust max_examples as needed.\n",
      "Processing up to 1000 examples...\n",
      "Processed 0 examples...\n",
      "Processed 100 examples...\n",
      "Processed 200 examples...\n",
      "Processed 300 examples...\n",
      "Processed 400 examples...\n",
      "Processed 500 examples...\n",
      "Processed 600 examples...\n",
      "Processed 700 examples...\n",
      "Processed 800 examples...\n",
      "Processed 900 examples...\n",
      "Completed processing 500 examples\n",
      "Successfully processed 500 documents\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Process the dataset\n",
    "print(\"Processing the dataset...\")\n",
    "print(\"Note: Processing 1000 examples. Adjust max_examples as needed.\")\n",
    "\n",
    "# Process the dataset (you can increase max_examples for more data)\n",
    "processed_data = process_dataset(dataset, max_examples=1000)\n",
    "print(f\"Successfully processed {len(processed_data)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "694530a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenized data...\n",
      "Data saved as:\n",
      "- gujarati_corpus_tokenized.json (JSON format)\n",
      "- gujarati_corpus_tokenized.pkl (Pickle format)\n",
      "- gujarati_corpus_tokenized_stats.txt (Statistics)\n",
      "Data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Save the tokenized data\n",
    "print(\"Saving tokenized data...\")\n",
    "\n",
    "# Save in multiple formats\n",
    "save_tokenized_data(processed_data, 'gujarati_corpus_tokenized')\n",
    "\n",
    "print(\"Data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "596e6860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing corpus statistics...\n",
      "\n",
      "============================================================\n",
      "CORPUS STATISTICS\n",
      "============================================================\n",
      "Total number of sentences                    : 1632\n",
      "Total number of words                        : 23504\n",
      "Total number of characters                   : 132007\n",
      "Average sentence length (words per sentence) : 14.4\n",
      "Average word length (characters per word)    : 4.63\n",
      "Type-Token Ratio (TTR)                       : 0.3609\n",
      "Total unique words                           : 8482\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Compute and display corpus statistics\n",
    "print(\"Computing corpus statistics...\")\n",
    "\n",
    "stats = compute_corpus_statistics(processed_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CORPUS STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key:45}: {value}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bec3f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Analysis of Processed Data\n",
      "========================================\n",
      "Sample document analysis:\n",
      "Original text length: 54 characters\n",
      "Number of sentences: 1\n",
      "Total words in document: 11\n",
      "First 100 characters: આ વીડિયો જુઓ: ઊંઝા માર્કેટયાર્ડ આજથી 25 જુલાઈ સુધી બંધ...\n",
      "\n",
      "First sentence details:\n",
      "Sentence text: આ વીડિયો જુઓ: ઊંઝા માર્કેટયાર્ડ આજથી 25 જુલાઈ સુધી બંધ\n",
      "Number of words: 11\n",
      "Words: ['આ', 'વીડિયો', 'જુઓ', ':', 'ઊંઝા', 'માર્કેટયાર્ડ', 'આજથી', '25', 'જુલાઈ', 'સુધી']...\n",
      "Token classifications (first 10):\n",
      "  'આ' -> gujarati_word\n",
      "  'વીડિયો' -> gujarati_word\n",
      "  'જુઓ' -> gujarati_word\n",
      "  ':' -> punctuation\n",
      "  'ઊંઝા' -> gujarati_word\n",
      "  'માર્કેટયાર્ડ' -> gujarati_word\n",
      "  'આજથી' -> gujarati_word\n",
      "  '25' -> integer\n",
      "  'જુલાઈ' -> gujarati_word\n",
      "  'સુધી' -> gujarati_word\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Detailed analysis of the processed data\n",
    "print(\"Detailed Analysis of Processed Data\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if processed_data:\n",
    "    # Sample document analysis\n",
    "    sample_doc = processed_data[0]\n",
    "    print(f\"Sample document analysis:\")\n",
    "    print(f\"Original text length: {len(sample_doc['original_text'])} characters\")\n",
    "    print(f\"Number of sentences: {len(sample_doc['sentences'])}\")\n",
    "    print(f\"Total words in document: {sample_doc['total_words']}\")\n",
    "    print(f\"First 100 characters: {sample_doc['original_text'][:100]}...\")\n",
    "    \n",
    "    print(f\"\\nFirst sentence details:\")\n",
    "    if sample_doc['sentences']:\n",
    "        first_sentence = sample_doc['sentences'][0]\n",
    "        print(f\"Sentence text: {first_sentence['text']}\")\n",
    "        print(f\"Number of words: {first_sentence['word_count']}\")\n",
    "        print(f\"Words: {first_sentence['words'][:10]}...\")  # First 10 words\n",
    "        \n",
    "        # Show token classifications for first sentence\n",
    "        print(f\"Token classifications (first 10):\")\n",
    "        for word, word_type in first_sentence['classified_words'][:10]:\n",
    "            print(f\"  '{word}' -> {word_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "847e02e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token Type Distribution Analysis\n",
      "========================================\n",
      "Token type distribution:\n",
      "gujarati_word       :    21576 (91.80%)\n",
      "punctuation         :     1381 (5.88%)\n",
      "integer             :      434 (1.85%)\n",
      "english_word        :      111 (0.47%)\n",
      "url                 :        2 (0.01%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Token type distribution analysis\n",
    "print(\"\\nToken Type Distribution Analysis\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Collect all token types\n",
    "all_token_types = []\n",
    "gujarati_words = []\n",
    "english_words = []\n",
    "numbers = []\n",
    "emails = []\n",
    "urls = []\n",
    "\n",
    "for doc in processed_data:\n",
    "    for sentence in doc['sentences']:\n",
    "        for word, word_type in sentence['classified_words']:\n",
    "            all_token_types.append(word_type)\n",
    "            \n",
    "            # Collect examples of different token types\n",
    "            if word_type == 'gujarati_word':\n",
    "                gujarati_words.append(word)\n",
    "            elif word_type == 'english_word':\n",
    "                english_words.append(word)\n",
    "            elif word_type in ['integer', 'decimal_number']:\n",
    "                numbers.append(word)\n",
    "            elif word_type == 'email':\n",
    "                emails.append(word)\n",
    "            elif word_type == 'url':\n",
    "                urls.append(word)\n",
    "\n",
    "# Count token types\n",
    "type_distribution = Counter(all_token_types)\n",
    "print(\"Token type distribution:\")\n",
    "for token_type, count in type_distribution.most_common():\n",
    "    percentage = (count / len(all_token_types)) * 100\n",
    "    print(f\"{token_type:20}: {count:8} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db86e73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SAMPLE TOKENS BY TYPE\n",
      "==================================================\n",
      "\n",
      "Sample Gujarati words (8185 unique):\n",
      "  ફાયદો\n",
      "  પોસઈ\n",
      "  નટ્ટૂ\n",
      "  ખાવા\n",
      "  ઉમેદવારો\n",
      "  પ્રોવિડન્ટ\n",
      "  ગણિત\n",
      "  પડેલી\n",
      "  ખોટું\n",
      "  ઈરાક\n",
      "\n",
      "Sample English words (99 unique):\n",
      "  first\n",
      "  RTGS\n",
      "  LAC\n",
      "  M\n",
      "  see\n",
      "  skewer\n",
      "  NEFT\n",
      "  Sehwag\n",
      "  ASEAN\n",
      "  BCCI\n",
      "\n",
      "Sample numbers (175 unique):\n",
      "  ૯૪\n",
      "  28\n",
      "  ૨૭\n",
      "  600\n",
      "  401\n",
      "  ૨૭૭૭\n",
      "  ૧૩૯\n",
      "  67\n",
      "  5\n",
      "  ૨૦૦૦\n",
      "\n",
      "URLs found (2 unique):\n",
      "  http://tiny\n",
      "  https://events\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Sample tokens by type\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE TOKENS BY TYPE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nSample Gujarati words ({len(set(gujarati_words))} unique):\")\n",
    "unique_gujarati = list(set(gujarati_words))[:10]\n",
    "for word in unique_gujarati:\n",
    "    print(f\"  {word}\")\n",
    "\n",
    "if english_words:\n",
    "    print(f\"\\nSample English words ({len(set(english_words))} unique):\")\n",
    "    unique_english = list(set(english_words))[:10]\n",
    "    for word in unique_english:\n",
    "        print(f\"  {word}\")\n",
    "\n",
    "if numbers:\n",
    "    print(f\"\\nSample numbers ({len(set(numbers))} unique):\")\n",
    "    unique_numbers = list(set(numbers))[:10]\n",
    "    for word in unique_numbers:\n",
    "        print(f\"  {word}\")\n",
    "\n",
    "if emails:\n",
    "    print(f\"\\nEmails found ({len(set(emails))} unique):\")\n",
    "    unique_emails = list(set(emails))[:5]\n",
    "    for email in unique_emails:\n",
    "        print(f\"  {email}\")\n",
    "\n",
    "if urls:\n",
    "    print(f\"\\nURLs found ({len(set(urls))} unique):\")\n",
    "    unique_urls = list(set(urls))[:5]\n",
    "    for url in unique_urls:\n",
    "        print(f\"  {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e9749b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ASSIGNMENT COMPLETION SUMMARY\n",
      "============================================================\n",
      "✓ Task A: Dataset downloaded and extracted\n",
      "✓ Task B: Sentence and word tokenizers implemented\n",
      "  - Handles Gujarati text with matras properly\n",
      "  - Tokenizes punctuation, URLs, numbers, emails, dates\n",
      "✓ Task C: Data saved in multiple formats:\n",
      "  - gujarati_corpus_tokenized.json\n",
      "  - gujarati_corpus_tokenized.pkl\n",
      "  - gujarati_corpus_tokenized_stats.txt\n",
      "✓ Task D: All required statistics computed:\n",
      "  - Total number of sentences: 1632\n",
      "  - Total number of words: 23504\n",
      "  - Total number of characters: 132007\n",
      "  - Average sentence length (words per sentence): 14.4\n",
      "  - Average word length (characters per word): 4.63\n",
      "  - Type-Token Ratio (TTR): 0.3609\n",
      "  - Total unique words: 8482\n",
      "\n",
      "All tasks completed successfully!\n",
      "\n",
      "File verification:\n",
      "✓ gujarati_corpus_tokenized.json: 3,678,244 bytes\n",
      "✓ gujarati_corpus_tokenized.pkl: 1,321,873 bytes\n",
      "✓ gujarati_corpus_tokenized_stats.txt: 258 bytes\n",
      "\n",
      "Contents of statistics file:\n",
      "Total number of sentences: 1632\n",
      "Total number of words: 23504\n",
      "Total number of characters: 132007\n",
      "Average sentence length (words per sentence): 14.4\n",
      "Average word length (characters per word): 4.63\n",
      "Type-Token Ratio (TTR): 0.3609\n",
      "Total unique words: 8482\n",
      "\n",
      "\n",
      "Processing complete! You can now submit your assignment.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Verify saved files and final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ASSIGNMENT COMPLETION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"✓ Task A: Dataset downloaded and extracted\")\n",
    "print(\"✓ Task B: Sentence and word tokenizers implemented\")\n",
    "print(\"  - Handles Gujarati text with matras properly\")\n",
    "print(\"  - Tokenizes punctuation, URLs, numbers, emails, dates\")\n",
    "print(\"✓ Task C: Data saved in multiple formats:\")\n",
    "print(\"  - gujarati_corpus_tokenized.json\")\n",
    "print(\"  - gujarati_corpus_tokenized.pkl\")\n",
    "print(\"  - gujarati_corpus_tokenized_stats.txt\")\n",
    "print(\"✓ Task D: All required statistics computed:\")\n",
    "\n",
    "# Display final statistics again\n",
    "for key, value in stats.items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "print(\"\\nAll tasks completed successfully!\")\n",
    "\n",
    "# Cell 11: Additional verification and file check\n",
    "import os\n",
    "\n",
    "print(\"\\nFile verification:\")\n",
    "files_to_check = [\n",
    "    'gujarati_corpus_tokenized.json',\n",
    "    'gujarati_corpus_tokenized.pkl', \n",
    "    'gujarati_corpus_tokenized_stats.txt'\n",
    "]\n",
    "\n",
    "for filename in files_to_check:\n",
    "    if os.path.exists(filename):\n",
    "        size = os.path.getsize(filename)\n",
    "        print(f\"✓ {filename}: {size:,} bytes\")\n",
    "    else:\n",
    "        print(f\"✗ {filename}: Not found\")\n",
    "\n",
    "# Show content of stats file\n",
    "print(\"\\nContents of statistics file:\")\n",
    "try:\n",
    "    with open('gujarati_corpus_tokenized_stats.txt', 'r', encoding='utf-8') as f:\n",
    "        print(f.read())\n",
    "except FileNotFoundError:\n",
    "    print(\"Statistics file not found\")\n",
    "\n",
    "print(\"\\nProcessing complete! You can now submit your assignment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
