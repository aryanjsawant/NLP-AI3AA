{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "308503d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gujarati_tokenizer import GujaratiTokenizer, process_dataset, save_tokenized_data, compute_corpus_statistics\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5675fff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gujarati dataset from IndicCorpV2...\n",
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Gujarati dataset from IndicCorpV2...\")\n",
    "dataset = load_dataset(\"ai4bharat/IndicCorpV2\", \"indiccorp_v2\", split=\"guj_Gujr\", streaming=True)\n",
    "print(\"Dataset loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "730a904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "આજે હું શાળાએ ગયો! મારું ઇમેઇલ student@gmail.com છે। \n",
      "આજની તારીખ 30/07/2025 છે અને સમય 10.30 વાગ્યે છે। \n",
      "વેબસાઇટ www.gujarati.com પર જાઓ। આ કિંમત ₹123.45 છે।\n",
      "\n",
      "==================================================\n",
      "Number of sentences: 10\n",
      "\n",
      "Sentences:\n",
      "1. આજે હું શાળાએ ગયો\n",
      "2. મારું ઇમેઇલ student@gmail\n",
      "3. com છે\n",
      "4. આજની તારીખ 30/07/2025 છે અને સમય 10\n",
      "5. 30 વાગ્યે છે\n",
      "6. વેબસાઇટ www\n",
      "7. gujarati\n",
      "8. com પર જાઓ\n",
      "9. આ કિંમત ₹123\n",
      "10. 45 છે\n",
      "\n",
      "==================================================\n",
      "Number of tokens: 34\n",
      "\n",
      "Tokens with classification:\n",
      "'આજે' -> gujarati_word\n",
      "'હું' -> gujarati_word\n",
      "'શાળાએ' -> gujarati_word\n",
      "'ગયો' -> gujarati_word\n",
      "'!' -> punctuation\n",
      "'મારું' -> gujarati_word\n",
      "'ઇમેઇલ' -> gujarati_word\n",
      "'student@gmail.com' -> email\n",
      "'છે' -> gujarati_word\n",
      "'।' -> punctuation\n",
      "'આજની' -> gujarati_word\n",
      "'તારીખ' -> gujarati_word\n",
      "'30' -> integer\n",
      "'/' -> punctuation\n",
      "'07' -> integer\n",
      "'/' -> punctuation\n",
      "'2025' -> integer\n",
      "'છે' -> gujarati_word\n",
      "'અને' -> gujarati_word\n",
      "'સમય' -> gujarati_word\n",
      "Processing the dataset...\n",
      "Note: Processing 1000 examples. Adjust max_examples as needed.\n",
      "Processing up to 1000 examples...\n",
      "Processed 0 examples...\n",
      "Processed 100 examples...\n",
      "Processed 200 examples...\n",
      "Processed 300 examples...\n",
      "Processed 400 examples...\n",
      "Processed 500 examples...\n",
      "Processed 600 examples...\n",
      "Processed 700 examples...\n",
      "Processed 800 examples...\n",
      "Processed 900 examples...\n",
      "Completed processing 500 examples\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GujaratiTokenizer()\n",
    "\n",
    "test_text = \"\"\"આજે હું શાળાએ ગયો! મારું ઇમેઇલ student@gmail.com છે। \n",
    "આજની તારીખ 30/07/2025 છે અને સમય 10.30 વાગ્યે છે। \n",
    "વેબસાઇટ www.gujarati.com પર જાઓ। આ કિંમત ₹123.45 છે।\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(test_text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "sentences = tokenizer.sentence_tokenize(test_text)\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "print(\"\\nSentences:\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sent.strip()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "words = tokenizer.word_tokenize(test_text)\n",
    "print(f\"Number of tokens: {len(words)}\")\n",
    "print(\"\\nTokens with classification:\")\n",
    "for word in words[:20]:\n",
    "    word_type = tokenizer.classify_token(word)\n",
    "    print(f\"'{word}' -> {word_type}\")\n",
    "\n",
    "print(\"Processing the dataset...\")\n",
    "print(\"Note: Processing 1000 examples. Adjust max_examples as needed.\")\n",
    "\n",
    "processed_data = process_dataset(dataset, max_examples=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1641488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 500 documents\n",
      "Saving tokenized data...\n",
      "Data saved as:\n",
      "- gujarati_corpus_tokenized.json (JSON format)\n",
      "- gujarati_corpus_tokenized.pkl (Pickle format)\n",
      "- gujarati_corpus_tokenized_stats.txt (Statistics)\n",
      "Data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Successfully processed {len(processed_data)} documents\")\n",
    "\n",
    "print(\"Saving tokenized data...\")\n",
    "save_tokenized_data(processed_data, 'gujarati_corpus_tokenized')\n",
    "print(\"Data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e48202b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing corpus statistics...\n",
      "\n",
      "============================================================\n",
      "CORPUS STATISTICS\n",
      "============================================================\n",
      "Total number of sentences                    : 1632\n",
      "Total number of words                        : 23504\n",
      "Total number of characters                   : 132007\n",
      "Average sentence length (words per sentence) : 14.4\n",
      "Average word length (characters per word)    : 4.63\n",
      "Type-Token Ratio (TTR)                       : 0.3609\n",
      "Total unique words                           : 8482\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Compute and display corpus statistics\n",
    "print(\"Computing corpus statistics...\")\n",
    "\n",
    "stats = compute_corpus_statistics(processed_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CORPUS STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key:45}: {value}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b73d048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Analysis of Processed Data\n",
      "========================================\n",
      "Sample document analysis:\n",
      "Original text length: 54 characters\n",
      "Number of sentences: 1\n",
      "Total words in document: 11\n",
      "First 100 characters: આ વીડિયો જુઓ: ઊંઝા માર્કેટયાર્ડ આજથી 25 જુલાઈ સુધી બંધ...\n",
      "\n",
      "First sentence details:\n",
      "Sentence text: આ વીડિયો જુઓ: ઊંઝા માર્કેટયાર્ડ આજથી 25 જુલાઈ સુધી બંધ\n",
      "Number of words: 11\n",
      "Words: ['આ', 'વીડિયો', 'જુઓ', ':', 'ઊંઝા', 'માર્કેટયાર્ડ', 'આજથી', '25', 'જુલાઈ', 'સુધી']...\n",
      "Token classifications (first 10):\n",
      "  'આ' -> gujarati_word\n",
      "  'વીડિયો' -> gujarati_word\n",
      "  'જુઓ' -> gujarati_word\n",
      "  ':' -> punctuation\n",
      "  'ઊંઝા' -> gujarati_word\n",
      "  'માર્કેટયાર્ડ' -> gujarati_word\n",
      "  'આજથી' -> gujarati_word\n",
      "  '25' -> integer\n",
      "  'જુલાઈ' -> gujarati_word\n",
      "  'સુધી' -> gujarati_word\n"
     ]
    }
   ],
   "source": [
    "print(\"Detailed Analysis of Processed Data\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if processed_data:\n",
    "    sample_doc = processed_data[0]\n",
    "    print(f\"Sample document analysis:\")\n",
    "    print(f\"Original text length: {len(sample_doc['original_text'])} characters\")\n",
    "    print(f\"Number of sentences: {len(sample_doc['sentences'])}\")\n",
    "    print(f\"Total words in document: {sample_doc['total_words']}\")\n",
    "    print(f\"First 100 characters: {sample_doc['original_text'][:100]}...\")\n",
    "    \n",
    "    print(f\"\\nFirst sentence details:\")\n",
    "    if sample_doc['sentences']:\n",
    "        first_sentence = sample_doc['sentences'][0]\n",
    "        print(f\"Sentence text: {first_sentence['text']}\")\n",
    "        print(f\"Number of words: {first_sentence['word_count']}\")\n",
    "        print(f\"Words: {first_sentence['words'][:10]}...\")\n",
    "        \n",
    "        print(f\"Token classifications (first 10):\")\n",
    "        for word, word_type in first_sentence['classified_words'][:10]:\n",
    "            print(f\"  '{word}' -> {word_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10808b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token Type Distribution Analysis\n",
      "========================================\n",
      "Token type distribution:\n",
      "gujarati_word       :    21576 (91.80%)\n",
      "punctuation         :     1381 (5.88%)\n",
      "integer             :      434 (1.85%)\n",
      "english_word        :      111 (0.47%)\n",
      "url                 :        2 (0.01%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nToken Type Distribution Analysis\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "all_token_types = []\n",
    "gujarati_words = []\n",
    "english_words = []\n",
    "numbers = []\n",
    "emails = []\n",
    "urls = []\n",
    "\n",
    "for doc in processed_data:\n",
    "    for sentence in doc['sentences']:\n",
    "        for word, word_type in sentence['classified_words']:\n",
    "            all_token_types.append(word_type)\n",
    "            \n",
    "            if word_type == 'gujarati_word':\n",
    "                gujarati_words.append(word)\n",
    "            elif word_type == 'english_word':\n",
    "                english_words.append(word)\n",
    "            elif word_type in ['integer', 'decimal_number']:\n",
    "                numbers.append(word)\n",
    "            elif word_type == 'email':\n",
    "                emails.append(word)\n",
    "            elif word_type == 'url':\n",
    "                urls.append(word)\n",
    "\n",
    "type_distribution = Counter(all_token_types)\n",
    "print(\"Token type distribution:\")\n",
    "for token_type, count in type_distribution.most_common():\n",
    "    percentage = (count / len(all_token_types)) * 100\n",
    "    print(f\"{token_type:20}: {count:8} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51c3dcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SAMPLE TOKENS BY TYPE\n",
      "==================================================\n",
      "\n",
      "Sample Gujarati words (8185 unique):\n",
      "  વહેતી\n",
      "  ઉપરાતં\n",
      "  અસાઇનમેન્ટની\n",
      "  ઉદયસિંહ\n",
      "  સ્મગલર\n",
      "  ભગત\n",
      "  સિંહ\n",
      "  સીસ્ટમ\n",
      "  લોડ\n",
      "  ફાયદો\n",
      "\n",
      "Sample English words (99 unique):\n",
      "  Shraddha\n",
      "  tablecloth\n",
      "  Neeraj\n",
      "  HalloApp\n",
      "  BCCI\n",
      "  Crorepati\n",
      "  RTGS\n",
      "  LED\n",
      "  Sledkov\n",
      "  GIDC\n",
      "\n",
      "Sample numbers (175 unique):\n",
      "  36\n",
      "  2021\n",
      "  ૧૫\n",
      "  08\n",
      "  ૦\n",
      "  ૭\n",
      "  ૪૦\n",
      "  65\n",
      "  ૬૦૦\n",
      "  ૬૦\n",
      "\n",
      "URLs found (2 unique):\n",
      "  https://events\n",
      "  http://tiny\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE TOKENS BY TYPE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nSample Gujarati words ({len(set(gujarati_words))} unique):\")\n",
    "unique_gujarati = list(set(gujarati_words))[:10]\n",
    "for word in unique_gujarati:\n",
    "    print(f\"  {word}\")\n",
    "\n",
    "if english_words:\n",
    "    print(f\"\\nSample English words ({len(set(english_words))} unique):\")\n",
    "    unique_english = list(set(english_words))[:10]\n",
    "    for word in unique_english:\n",
    "        print(f\"  {word}\")\n",
    "\n",
    "if numbers:\n",
    "    print(f\"\\nSample numbers ({len(set(numbers))} unique):\")\n",
    "    unique_numbers = list(set(numbers))[:10]\n",
    "    for word in unique_numbers:\n",
    "        print(f\"  {word}\")\n",
    "\n",
    "if emails:\n",
    "    print(f\"\\nEmails found ({len(set(emails))} unique):\")\n",
    "    unique_emails = list(set(emails))[:5]\n",
    "    for email in unique_emails:\n",
    "        print(f\"  {email}\")\n",
    "\n",
    "if urls:\n",
    "    print(f\"\\nURLs found ({len(set(urls))} unique):\")\n",
    "    unique_urls = list(set(urls))[:5]\n",
    "    for url in unique_urls:\n",
    "        print(f\"  {url}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ddc4f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File verification:\n",
      "✓ gujarati_corpus_tokenized.json: 3,678,244 bytes\n",
      "✓ gujarati_corpus_tokenized.pkl: 1,321,873 bytes\n",
      "✓ gujarati_corpus_tokenized_stats.txt: 258 bytes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nFile verification:\")\n",
    "files_to_check = [\n",
    "    'gujarati_corpus_tokenized.json',\n",
    "    'gujarati_corpus_tokenized.pkl', \n",
    "    'gujarati_corpus_tokenized_stats.txt'\n",
    "]\n",
    "\n",
    "for filename in files_to_check:\n",
    "    if os.path.exists(filename):\n",
    "        size = os.path.getsize(filename)\n",
    "        print(f\"✓ {filename}: {size:,} bytes\")\n",
    "    else:\n",
    "        print(f\"✗ {filename}: Not found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
