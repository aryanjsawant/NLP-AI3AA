{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf968510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aryan\\Documents\\GitHub\\nlp-ai3aa\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Telugu subset of the dataset\n",
    "dataset = load_dataset(\"ai4bharat/IndicCorpV2\", \"indiccorp_v2\", split=\"gom_Deva\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d959c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset = dataset.filter(lambda x: x[\"text\"].strip() != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85e265de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "संस्मरणीय अनुभव!\n",
      "Bible\n",
      "@गवि, अतिशय धन्यवाद माहितीसाठी. मला बाने ला जाउन  ३-४ वर्षे झाली आता. त्यामुळं काही नीटसं लक्षात नाही.  तेंव्हा तिथं झेंड्याची सिस्टिम होती. फार भारी वाटला होता तो प्रकार.\n",
      "तर काहीजण चक्रव्युहाची रचनाच बदलायला निघतात...\n",
      "Lokxmichi vhoddli bhoinn. Tika ‘jyexttha’ hem vegllem namv asa. Moharaxttrant tika ‘akkabai’ mhunn vollkhotat. Olkxmi hea utrache orth ji lokxmi nhoi ti oso zata.Kxirsagorachea somudromonthona vellar vikh (kalkutt) bhair sortokoch, punn lokxmichea poilim doreantlean bhair sorli mhunn tika lokxmichi vhoddli bhoinn manpak lagle. Olkxmi mhollear daridry, doiny ani dukhkh. Ti doreantlean bhair yetkoch tinnem vicharolem, ‘hamv khoim ravum?’ tacher devamni tika sanglem, ‘zom-i kellso, kunddo ani kems astat thoim tum ravo.’‘jea ghorant zhogddim, zhuzam, dukhkham hanche rabito asa. Guru devo, soire dhaire ixtt hancho opman zata. Zom-i fott uloitat, zom-i lok ekamekank fottoitat. Zom-i odhrmacheo gozali choltat thointum ravo.’\n",
      "३. प्रशिक्षणासाठी अमेरिकेत गेलेल्या विश्वास मोहपात्रा या भारतीय वन सेवेतील अधिका-याला अमेरिकेत अटक झाली. (सकाळ २७ जुलै २०१२)\n",
      "मेघना भुस्कुटे: 'स्त्री' असण्यासोबत कोणकोणत्या गोष्टी आयुष्यात न मागता येतात - यांत सकारात्मक आणि नकारात्मक दोन्ही गोष्टींचा समावेश आहे, त्याकडे आपण कसं पाहतो, त्यावर मात कशी करतो / त्यांच्याशी जमवून कसं घेतो या आणि अशा गोष्टींवरही लिहिलं जावं, असं वाटतं.\n",
      "करिघृतडमरूतुनी उपजति ज्ञान कला\n",
      "आहे. त्यानंतर आलेला त्यांचा स्वदेस हा चित्रपटही असाच नाविन्यपूर्ण व अजब घटना दाखवणारा होता. तोही सुपरहिट झाला. त्यामुळे त्यानंतर आलेल्या जोधा अकबर या सिनेमाबद्दल तो प्रदर्शित होण्यापूर्वीच मोठ्या अपेक्षा निर्माण झाल्या होत्या. त्या बहुतांशी\n",
      "सध्याच्या व्हायरस मुळे पुढच्या काही वर्षांत ज्या एका प्रकारच्या भीतीचं प्रमाण वाढणार आहे तो प्रकार म्हणजे हेल्थ anxiety. या रोगाला पूर्वी हायपोकोंड्रिया म्हटलं जात असे. हेल्थ anxiety च्या व्यक्तींना स्वतःच्या आरोग्याविषयी खूपच चिंता वाटत असते. आपल्याला हा रोग तर नाही ना तो रोग तर नाही ना अश्या अवाजवी काळजीने त्यांचं मन सतत ग्रासलेलं असतं.\n",
      "98115 वाचने\n",
      "पारणे म्हणजे काय? डोळ्याचे पारणे फिटणे........म्हणजे नेत्रसूख अनुभवणे ( फर्गुसन रोड वर अनुभवास होते तसे नव्हे)\n",
      "गांवांत व्हॅन उपलब्ध ना.सगळ्यांत लागीं व्हॅन ५ ते १० किलोमिटराच्या अंतराचेर आसा.\n",
      "चला तर मग.\n",
      "केलं ते चूक का बरोबर हे तपशील माहिती नसतानाच सांगणं शक्य नाही. आणि एनी वेज् ते करुन झालेलं आहे.\n",
      "सगळात वरचे दृश्य म्हणजे - शेवटचे \" मोठे जहाज\" आणि त्यावरचे प्रसंग.\n",
      "तांचे पुरस्कार आनी साहित्य संपदा.\n",
      "विजेवर चालणार हा पंप नळ सुरु केला कि सुरु होतो आणि बंद केल्यावर थांबतो. वीज पण जास्त जळत नाही.\n",
      "लक्षव्दिप भारताचो एक केंद्रशासीत प्रदेश. अरबी दर्यांतल्या साबार जुंव्यांचो मेळून जाल्लो हो प्रदेश. ह्या ल्हान-व्हड जुंव्यांचें वट्ट क्षेत्रफळ 32 चौ. किमी. आसा. ह्या शिंवरून पडिल्ल्या जुंव्यांचो विस्तार 8° ते 12°13’ उत्तर अक्षांश आनी 71° ते 74° उदेंत रेखांश. तांतुतलो अँड्रोत हो सगळ्यांत व्हडलो आसून ताचें क्षेत्रफळ 4.8 चौ. किमी. आसा. हेर मुखेल जुंव्यांभितर अमिनी, अगत्ती, बित्रा, चेटलट, काडमट, काल्पेनी, कावारट्टी, किल्टन आनी मिनिकॉय हांचो आस्पाव जाता. हातुंतल्या खंयच्याच जुंव्याची रुंदाय 2 किमी. वयर ना. हे जुंवे केरळ राज्यांतल्या कोची शारासावन सुमार 220 ते 440 किमी. पयस आसात. कारवारट्टी हें प्रदेशाचें राजपाटण.\n",
      "प्रमेय in जनातलं, मनातलं\n",
      "६.हाच तो ताकीन\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(cleaned_dataset):\n",
    "    print(example[\"text\"])\n",
    "    if i >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5e750f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1361209"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count=0\n",
    "for i, example in enumerate(cleaned_dataset):\n",
    "    count=count+1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8009f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_sentences = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5675fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries and the tokenizer\n",
    "\n",
    "\n",
    "# Cell 2: Load the Gujarati dataset\n",
    "print(\"Loading Gujarati dataset from IndicCorpV2...\")\n",
    "dataset = load_dataset(\"ai4bharat/IndicCorpV2\", \"indiccorp_v2\", split=\"guj_Gujr\", streaming=True)\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "# Cell 3: Test the tokenizer with sample data\n",
    "print(\"Testing tokenizer with sample Gujarati text...\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = GujaratiTokenizer()\n",
    "\n",
    "# Test with sample Gujarati text containing various elements\n",
    "test_text = \"\"\"આજે હું શાળાએ ગયો! મારું ઇમેઇલ student@gmail.com છે। \n",
    "આજની તારીખ 30/07/2025 છે અને સમય 10.30 વાગ્યે છે। \n",
    "વેબસાઇટ www.gujarati.com પર જાઓ। આ કિંમત ₹123.45 છે।\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(test_text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Test sentence tokenization\n",
    "sentences = tokenizer.sentence_tokenize(test_text)\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "print(\"\\nSentences:\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sent.strip()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Test word tokenization\n",
    "words = tokenizer.word_tokenize(test_text)\n",
    "print(f\"Number of tokens: {len(words)}\")\n",
    "print(\"\\nTokens with classification:\")\n",
    "for word in words[:20]:  # Show first 20 tokens\n",
    "    word_type = tokenizer.classify_token(word)\n",
    "    print(f\"'{word}' -> {word_type}\")\n",
    "\n",
    "# Cell 4: Process the dataset\n",
    "print(\"Processing the dataset...\")\n",
    "print(\"Note: Processing 1000 examples. Adjust max_examples as needed.\")\n",
    "\n",
    "# Process the dataset (you can increase max_examples for more data)\n",
    "processed_data = process_dataset(dataset, max_examples=1000)\n",
    "\n",
    "print(f\"Successfully processed {len(processed_data)} documents\")\n",
    "\n",
    "# Cell 5: Save the tokenized data\n",
    "print(\"Saving tokenized data...\")\n",
    "\n",
    "# Save in multiple formats\n",
    "save_tokenized_data(processed_data, 'gujarati_corpus_tokenized')\n",
    "\n",
    "print(\"Data saved successfully!\")\n",
    "\n",
    "# Cell 6: Compute and display corpus statistics\n",
    "print(\"Computing corpus statistics...\")\n",
    "\n",
    "stats = compute_corpus_statistics(processed_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CORPUS STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key:45}: {value}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cell 7: Detailed analysis of the processed data\n",
    "print(\"Detailed Analysis of Processed Data\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if processed_data:\n",
    "    # Sample document analysis\n",
    "    sample_doc = processed_data[0]\n",
    "    print(f\"Sample document analysis:\")\n",
    "    print(f\"Original text length: {len(sample_doc['original_text'])} characters\")\n",
    "    print(f\"Number of sentences: {len(sample_doc['sentences'])}\")\n",
    "    print(f\"Total words in document: {sample_doc['total_words']}\")\n",
    "    print(f\"First 100 characters: {sample_doc['original_text'][:100]}...\")\n",
    "    \n",
    "    print(f\"\\nFirst sentence details:\")\n",
    "    if sample_doc['sentences']:\n",
    "        first_sentence = sample_doc['sentences'][0]\n",
    "        print(f\"Sentence text: {first_sentence['text']}\")\n",
    "        print(f\"Number of words: {first_sentence['word_count']}\")\n",
    "        print(f\"Words: {first_sentence['words'][:10]}...\")  # First 10 words\n",
    "        \n",
    "        # Show token classifications for first sentence\n",
    "        print(f\"Token classifications (first 10):\")\n",
    "        for word, word_type in first_sentence['classified_words'][:10]:\n",
    "            print(f\"  '{word}' -> {word_type}\")\n",
    "\n",
    "# Cell 8: Token type distribution analysis\n",
    "print(\"\\nToken Type Distribution Analysis\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Collect all token types\n",
    "all_token_types = []\n",
    "gujarati_words = []\n",
    "english_words = []\n",
    "numbers = []\n",
    "emails = []\n",
    "urls = []\n",
    "\n",
    "for doc in processed_data:\n",
    "    for sentence in doc['sentences']:\n",
    "        for word, word_type in sentence['classified_words']:\n",
    "            all_token_types.append(word_type)\n",
    "            \n",
    "            # Collect examples of different token types\n",
    "            if word_type == 'gujarati_word':\n",
    "                gujarati_words.append(word)\n",
    "            elif word_type == 'english_word':\n",
    "                english_words.append(word)\n",
    "            elif word_type in ['integer', 'decimal_number']:\n",
    "                numbers.append(word)\n",
    "            elif word_type == 'email':\n",
    "                emails.append(word)\n",
    "            elif word_type == 'url':\n",
    "                urls.append(word)\n",
    "\n",
    "# Count token types\n",
    "type_distribution = Counter(all_token_types)\n",
    "print(\"Token type distribution:\")\n",
    "for token_type, count in type_distribution.most_common():\n",
    "    percentage = (count / len(all_token_types)) * 100\n",
    "    print(f\"{token_type:20}: {count:8} ({percentage:.2f}%)\")\n",
    "\n",
    "# Cell 9: Sample tokens by type\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE TOKENS BY TYPE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nSample Gujarati words ({len(set(gujarati_words))} unique):\")\n",
    "unique_gujarati = list(set(gujarati_words))[:10]\n",
    "for word in unique_gujarati:\n",
    "    print(f\"  {word}\")\n",
    "\n",
    "if english_words:\n",
    "    print(f\"\\nSample English words ({len(set(english_words))} unique):\")\n",
    "    unique_english = list(set(english_words))[:10]\n",
    "    for word in unique_english:\n",
    "        print(f\"  {word}\")\n",
    "\n",
    "if numbers:\n",
    "    print(f\"\\nSample numbers ({len(set(numbers))} unique):\")\n",
    "    unique_numbers = list(set(numbers))[:10]\n",
    "    for word in unique_numbers:\n",
    "        print(f\"  {word}\")\n",
    "\n",
    "if emails:\n",
    "    print(f\"\\nEmails found ({len(set(emails))} unique):\")\n",
    "    unique_emails = list(set(emails))[:5]\n",
    "    for email in unique_emails:\n",
    "        print(f\"  {email}\")\n",
    "\n",
    "if urls:\n",
    "    print(f\"\\nURLs found ({len(set(urls))} unique):\")\n",
    "    unique_urls = list(set(urls))[:5]\n",
    "    for url in unique_urls:\n",
    "        print(f\"  {url}\")\n",
    "\n",
    "# Cell 10: Verify saved files and final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ASSIGNMENT COMPLETION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"✓ Task A: Dataset downloaded and extracted\")\n",
    "print(\"✓ Task B: Sentence and word tokenizers implemented\")\n",
    "print(\"  - Handles Gujarati text with matras properly\")\n",
    "print(\"  - Tokenizes punctuation, URLs, numbers, emails, dates\")\n",
    "print(\"✓ Task C: Data saved in multiple formats:\")\n",
    "print(\"  - gujarati_corpus_tokenized.json\")\n",
    "print(\"  - gujarati_corpus_tokenized.pkl\")\n",
    "print(\"  - gujarati_corpus_tokenized_stats.txt\")\n",
    "print(\"✓ Task D: All required statistics computed:\")\n",
    "\n",
    "# Display final statistics again\n",
    "for key, value in stats.items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "print(\"\\nAll tasks completed successfully!\")\n",
    "\n",
    "# Cell 11: Additional verification and file check\n",
    "import os\n",
    "\n",
    "print(\"\\nFile verification:\")\n",
    "files_to_check = [\n",
    "    'gujarati_corpus_tokenized.json',\n",
    "    'gujarati_corpus_tokenized.pkl', \n",
    "    'gujarati_corpus_tokenized_stats.txt'\n",
    "]\n",
    "\n",
    "for filename in files_to_check:\n",
    "    if os.path.exists(filename):\n",
    "        size = os.path.getsize(filename)\n",
    "        print(f\"✓ {filename}: {size:,} bytes\")\n",
    "    else:\n",
    "        print(f\"✗ {filename}: Not found\")\n",
    "\n",
    "# Show content of stats file\n",
    "print(\"\\nContents of statistics file:\")\n",
    "try:\n",
    "    with open('gujarati_corpus_tokenized_stats.txt', 'r', encoding='utf-8') as f:\n",
    "        print(f.read())\n",
    "except FileNotFoundError:\n",
    "    print(\"Statistics file not found\")\n",
    "\n",
    "print(\"\\nProcessing complete! You can now submit your assignment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
