{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb9fa6e",
   "metadata": {},
   "source": [
    "# Lab 6 – Advanced Language Models and Generation\n",
    "\n",
    "This notebook implements:\n",
    "\n",
    "- Katz Backoff for a 4-gram model\n",
    "- Interpolated Kneser–Ney smoothing for a 4-gram model\n",
    "- Sentence generation (100 sentences) for n-grams n∈{1,2,3,4} using:\n",
    "  - Greedy (MLE)\n",
    "  - Beam search (beam=20)\n",
    "\n",
    "It builds on the tokenized Gujarati dataset prepared in Lab 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a831e176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences available: 1631\n",
      "Split sizes -> train: 1431, val: 100, test: 100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "BOS = \"<s>\"\n",
    "EOS = \"</s>\"\n",
    "\n",
    "# Load tokenized dataset (from lab1)\n",
    "with open(\"../lab1/gujarati_corpus_tokenized.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract sentences as token lists (keeping punctuation tokens if present)\n",
    "all_sentences_tokens: List[List[str]] = []\n",
    "for doc in data:\n",
    "    for s in doc.get(\"sentences\", []):\n",
    "        tokens = s.get(\"words\", [])\n",
    "        if tokens:\n",
    "            all_sentences_tokens.append(tokens)\n",
    "\n",
    "num_sentences = len(all_sentences_tokens)\n",
    "print(f\"Total sentences available: {num_sentences}\")\n",
    "\n",
    "# Create 100/100/remaining splits\n",
    "indices = list(range(num_sentences))\n",
    "random.shuffle(indices)\n",
    "val_size = min(100, num_sentences)\n",
    "test_size = min(100, max(0, num_sentences - val_size))\n",
    "val_idx = set(indices[:val_size])\n",
    "test_idx = set(indices[val_size:val_size + test_size])\n",
    "\n",
    "val_set = [all_sentences_tokens[i] for i in val_idx]\n",
    "test_set = [all_sentences_tokens[i] for i in test_idx]\n",
    "train_set = [all_sentences_tokens[i] for i in indices if i not in val_idx and i not in test_idx]\n",
    "\n",
    "print(f\"Split sizes -> train: {len(train_set)}, val: {len(val_set)}, test: {len(test_set)}\")\n",
    "\n",
    "# Wrap sentences with BOS/EOS for n-gram building\n",
    "train_wrapped = [[BOS, BOS, BOS] + s + [EOS] for s in train_set]\n",
    "val_wrapped = [[BOS, BOS, BOS] + s + [EOS] for s in val_set]\n",
    "test_wrapped = [[BOS, BOS, BOS] + s + [EOS] for s in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09aafcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 7701\n",
      "Some 4-gram examples: [(('<s>', '<s>', '<s>', 'રજાઓ'), 1), (('<s>', '<s>', 'રજાઓ', 'અને'), 1), (('<s>', 'રજાઓ', 'અને', 'સપ્તાહના'), 1)]\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(sentences: List[List[str]]) -> Counter:\n",
    "    vocab = Counter()\n",
    "    for s in sentences:\n",
    "        vocab.update(s)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def ngrams(seq: List[str], n: int) -> List[Tuple[str, ...]]:\n",
    "    return [tuple(seq[i:i+n]) for i in range(len(seq) - n + 1)]\n",
    "\n",
    "\n",
    "def build_ngram_counts(sentences: List[List[str]], max_n: int = 4):\n",
    "    counts = {n: Counter() for n in range(1, max_n + 1)}\n",
    "    for s in sentences:\n",
    "        for n in range(1, max_n + 1):\n",
    "            counts[n].update(ngrams(s, n))\n",
    "    return counts\n",
    "\n",
    "\n",
    "vocab = build_vocab(train_wrapped)\n",
    "V = len(vocab)\n",
    "counts = build_ngram_counts(train_wrapped, 4)\n",
    "\n",
    "# Precompute context counts for conditional probabilities\n",
    "context_counts = {n: Counter() for n in range(2, 5)}  # for n>=2\n",
    "for n in range(2, 5):\n",
    "    for ng, c in counts[n].items():\n",
    "        ctx = ng[:-1]\n",
    "        context_counts[n][ctx] += c\n",
    "\n",
    "print(\"Vocab size:\", V)\n",
    "print(\"Some 4-gram examples:\", list(counts[4].items())[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e507cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Katz Backoff (Good-Turing based) for up to 4-grams\n",
    "\n",
    "def good_turing_discount(c: int, Nc: Dict[int, int]):\n",
    "    # Simple Good-Turing: c* = (c+1) * N_{c+1}/N_c if available, else c\n",
    "    if c == 0:\n",
    "        return 0.0\n",
    "    Nc_c = Nc.get(c, 0)\n",
    "    Nc_cp1 = Nc.get(c + 1, 0)\n",
    "    if Nc_c > 0 and Nc_cp1 > 0:\n",
    "        return (c + 1) * (Nc_cp1 / Nc_c)\n",
    "    return float(c)\n",
    "\n",
    "\n",
    "def count_of_counts(counter: Counter) -> Dict[int, int]:\n",
    "    Nc = Counter(counter.values())\n",
    "    return dict(Nc)\n",
    "\n",
    "\n",
    "Nc_tables = {n: count_of_counts(counts[n]) for n in range(1, 5)}\n",
    "\n",
    "\n",
    "def katz_prob(word: str, context: Tuple[str, ...]):\n",
    "    # context length can be 0..3. We try highest order then backoff.\n",
    "    n = len(context) + 1\n",
    "    if n == 1:\n",
    "        # Unigram: MLE with add-small epsilon to avoid zero\n",
    "        total = sum(counts[1].values())\n",
    "        return counts[1][(word,)] / total if total else 1.0 / max(1, V)\n",
    "\n",
    "    # Attempt n-gram\n",
    "    full = context + (word,)\n",
    "    c_full = counts[n][full]\n",
    "    c_ctx = context_counts[n][context] if n >= 2 else sum(counts[1].values())\n",
    "\n",
    "    if c_full > 0 and c_ctx > 0:\n",
    "        # Discounted probability\n",
    "        c_star = good_turing_discount(c_full, Nc_tables[n])\n",
    "        return c_star / c_ctx\n",
    "\n",
    "    # Backoff mass alpha(context)\n",
    "    # alpha = leftover mass for unseen continuations of this context\n",
    "    # alpha = 1 - sum_{w: c(context,w)>0} discounted_prob(context,w)\n",
    "    if c_ctx == 0:\n",
    "        # no evidence for this context -> back off immediately\n",
    "        shorter = context[1:] if len(context) > 0 else ()\n",
    "        return katz_prob(word, shorter)\n",
    "\n",
    "    seen_words = [full_w[-1] for full_w in counts[n] if full_w[:-1] == context]\n",
    "    mass_seen = 0.0\n",
    "    for w in seen_words:\n",
    "        c = counts[n][context + (w,)]\n",
    "        c_star = good_turing_discount(c, Nc_tables[n])\n",
    "        mass_seen += c_star / c_ctx\n",
    "    alpha = max(0.0, 1.0 - mass_seen)\n",
    "\n",
    "    # Normalization denominator for backoff distribution over unseen words\n",
    "    # denom = sum of lower-order probabilities for words not seen in this context\n",
    "    shorter = context[1:] if len(context) > 0 else ()\n",
    "    denom = 0.0\n",
    "    for w in vocab.keys():\n",
    "        if counts[n][context + (w,)] == 0:\n",
    "            denom += katz_prob(w, shorter)\n",
    "    if denom == 0:\n",
    "        # fall back to uniform tiny prob\n",
    "        return alpha / V\n",
    "    return alpha * (katz_prob(word, shorter) / denom)\n",
    "\n",
    "\n",
    "def sentence_logprob_katz(tokens: List[str]) -> float:\n",
    "    seq = [BOS, BOS, BOS] + tokens + [EOS]\n",
    "    logp = 0.0\n",
    "    for i in range(3, len(seq)):\n",
    "        ctx = tuple(seq[i-3:i])\n",
    "        w = seq[i]\n",
    "        p = max(1e-12, katz_prob(w, ctx))\n",
    "        logp += math.log(p)\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb634366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolated Kneser-Ney for up to 4-grams\n",
    "\n",
    "D = 0.75  # standard discount\n",
    "\n",
    "# Precompute: number of unique continuations and unique histories\n",
    "unique_continuations = {n: defaultdict(set) for n in range(2, 5)}  # context -> set of next words\n",
    "unique_histories = {n: defaultdict(set) for n in range(2, 5)}      # word -> set of previous contexts\n",
    "\n",
    "for n in range(2, 5):\n",
    "    for ng, c in counts[n].items():\n",
    "        ctx, w = ng[:-1], ng[-1]\n",
    "        unique_continuations[n][ctx].add(w)\n",
    "        unique_histories[n][w].add(ctx)\n",
    "\n",
    "# For unigrams: Kneser-Ney base uses continuation counts\n",
    "total_unique_bigrams = len(counts[2]) if counts[2] else 1\n",
    "\n",
    "\n",
    "def P_continuation(w: str) -> float:\n",
    "    # number of distinct contexts that w follows divided by total distinct bigrams\n",
    "    return len(unique_histories[2].get(w, set())) / total_unique_bigrams\n",
    "\n",
    "\n",
    "def kn_prob(word: str, context: Tuple[str, ...]) -> float:\n",
    "    m = len(context)\n",
    "    if m == 0:\n",
    "        # unigram KN\n",
    "        p = P_continuation(word)\n",
    "        if p == 0:\n",
    "            return 1.0 / V\n",
    "        return p\n",
    "    # use order m+1 model\n",
    "    n = m + 1\n",
    "    c_ctx = context_counts[n][context]\n",
    "    c_full = counts[n][context + (word,)]\n",
    "    if c_ctx > 0:\n",
    "        # discounted MLE part\n",
    "        discounted = max(c_full - D, 0) / c_ctx\n",
    "        # backoff weight\n",
    "        lambda_ctx = (D * len(unique_continuations[n].get(context, set()))) / c_ctx\n",
    "        return discounted + lambda_ctx * kn_prob(word, context[1:])\n",
    "    else:\n",
    "        # no evidence for context; back off\n",
    "        return kn_prob(word, context[1:])\n",
    "\n",
    "\n",
    "def sentence_logprob_kn(tokens: List[str]) -> float:\n",
    "    seq = [BOS, BOS, BOS] + tokens + [EOS]\n",
    "    logp = 0.0\n",
    "    for i in range(3, len(seq)):\n",
    "        ctx = tuple(seq[i-3:i])\n",
    "        w = seq[i]\n",
    "        p = max(1e-12, kn_prob(w, ctx))\n",
    "        logp += math.log(p)\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a785b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "MAX_LEN = 30\n",
    "\n",
    "\n",
    "def mle_prob(word: str, context: Tuple[str, ...]) -> float:\n",
    "    n = len(context) + 1\n",
    "    if n == 1:\n",
    "        total = sum(counts[1].values())\n",
    "        return counts[1][(word,)] / total if total else 1.0 / max(1, V)\n",
    "    c_ctx = context_counts[n][context]\n",
    "    if c_ctx == 0:\n",
    "        return mle_prob(word, context[1:])\n",
    "    return counts[n][context + (word,)] / c_ctx\n",
    "\n",
    "\n",
    "def greedy_generate(n: int, model: str = \"mle\") -> List[str]:\n",
    "    seq = [BOS, BOS, BOS]\n",
    "    for _ in range(MAX_LEN):\n",
    "        context = tuple(seq[-(n-1):]) if n > 1 else tuple()\n",
    "        best_w = None\n",
    "        best_p = -1.0\n",
    "        for w in vocab.keys():\n",
    "            if w == BOS:\n",
    "                continue\n",
    "            if model == \"mle\":\n",
    "                p = mle_prob(w, context)\n",
    "            elif model == \"katz\":\n",
    "                p = katz_prob(w, context)\n",
    "            elif model == \"kn\":\n",
    "                p = kn_prob(w, context)\n",
    "            else:\n",
    "                p = mle_prob(w, context)\n",
    "            if p > best_p:\n",
    "                best_p = p\n",
    "                best_w = w\n",
    "        if best_w is None or best_w == EOS:\n",
    "            break\n",
    "        seq.append(best_w)\n",
    "        if best_w == EOS:\n",
    "            break\n",
    "    # Trim BOS and stop at EOS\n",
    "    out = [t for t in seq[3:] if t != EOS]\n",
    "    return out\n",
    "\n",
    "\n",
    "def beam_search_generate(n: int, beam_size: int = 20, model: str = \"mle\") -> List[str]:\n",
    "    # each hypothesis: (neg_logp, seq)\n",
    "    start = [BOS, BOS, BOS]\n",
    "    beam = [(0.0, start)]\n",
    "    completed = []\n",
    "    for _ in range(MAX_LEN):\n",
    "        new_beam = []\n",
    "        for neg_logp, seq in beam:\n",
    "            context = tuple(seq[-(n-1):]) if n > 1 else tuple()\n",
    "            for w in vocab.keys():\n",
    "                if w == BOS:\n",
    "                    continue\n",
    "                if model == \"mle\":\n",
    "                    p = mle_prob(w, context)\n",
    "                elif model == \"katz\":\n",
    "                    p = katz_prob(w, context)\n",
    "                elif model == \"kn\":\n",
    "                    p = kn_prob(w, context)\n",
    "                else:\n",
    "                    p = mle_prob(w, context)\n",
    "                p = max(p, 1e-12)\n",
    "                new_seq = seq + [w]\n",
    "                new_score = neg_logp - math.log(p)\n",
    "                if w == EOS:\n",
    "                    completed.append((new_score, new_seq))\n",
    "                else:\n",
    "                    new_beam.append((new_score, new_seq))\n",
    "        # prune\n",
    "        beam = sorted(new_beam)[:beam_size]\n",
    "        if not beam:\n",
    "            break\n",
    "    if completed:\n",
    "        best = min(completed, key=lambda x: x[0])[1]\n",
    "    else:\n",
    "        best = min(beam, key=lambda x: x[0])[1]\n",
    "    out = [t for t in best[3:] if t != EOS]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c442e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 100 sentences for n=1..4 using Greedy (MLE), Katz, and Kneser-Ney\n",
    "\n",
    "generated = {\"greedy_mle\": {}, \"greedy_katz\": {}, \"greedy_kn\": {},\n",
    "             \"beam_mle\": {}, \"beam_katz\": {}, \"beam_kn\": {}}\n",
    "\n",
    "for n in [1, 2, 3, 4]:\n",
    "    greedy_mle_list = [\" \".join(greedy_generate(n, model=\"mle\")) for _ in range(100)]\n",
    "    greedy_katz_list = [\" \".join(greedy_generate(n, model=\"katz\")) for _ in range(100)]\n",
    "    greedy_kn_list = [\" \".join(greedy_generate(n, model=\"kn\")) for _ in range(100)]\n",
    "\n",
    "    beam_mle_list = [\" \".join(beam_search_generate(n, 20, model=\"mle\")) for _ in range(100)]\n",
    "    beam_katz_list = [\" \".join(beam_search_generate(n, 20, model=\"katz\")) for _ in range(100)]\n",
    "    beam_kn_list = [\" \".join(beam_search_generate(n, 20, model=\"kn\")) for _ in range(100)]\n",
    "\n",
    "    generated[\"greedy_mle\"][n] = greedy_mle_list\n",
    "    generated[\"greedy_katz\"][n] = greedy_katz_list\n",
    "    generated[\"greedy_kn\"][n] = greedy_kn_list\n",
    "    generated[\"beam_mle\"][n] = beam_mle_list\n",
    "    generated[\"beam_katz\"][n] = beam_katz_list\n",
    "    generated[\"beam_kn\"][n] = beam_kn_list\n",
    "\n",
    "# Show few samples\n",
    "for key in [\"greedy_mle\", \"greedy_katz\", \"greedy_kn\", \"beam_mle\", \"beam_katz\", \"beam_kn\"]:\n",
    "    print(f\"\\n=== {key} ===\")\n",
    "    for n in [1, 2, 3, 4]:\n",
    "        print(f\"n={n} -> {generated[key][n][0][:200]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
